import numpy as np
import logging
from src.decorators import dgp, method
import jax
from jax import numpy as jnp
from jax import lax
from jax.scipy.linalg import cho_solve


from collections import namedtuple

logger = logging.getLogger(__name__)

semiBayesResults = namedtuple("semiBayesResults", ["beta_tilde", "C_tilde"])
mle_results = namedtuple("mle_results", ["beta_hat", "V_hat"])
parametricEBResults = namedtuple(
    "parametricEBResults", ["beta_star", "C_star", "tau_tilde2"]
)


def sigmoid(x):
    return jnp.exp(x) / (1 + jnp.exp(x))


@dgp(output=["X", "y", "beta"], label="greenland")
def generate_design_matrix(prng_key, N, n, rho=0.0, sigma2=1.0, tau0=1.0, tau1=1.0):
    """
    Generate the design matrix X and binary response vector y as outlined
    in Greenland (1993). The design matrix X is a binary matrix of u
    exposures, generated by thresholding correlated normal variables Z.
    Additional noise is added over the standard binomial variance for
    y | X. Finally, the alpha intercept is chosen to center the logits
    and ensure that roughly 50% of the responses are 1s.
    """
    beta_key, data_key = jax.random.split(prng_key, 2)

    # generate true regression coefficients
    bk1, bk2, bk3 = jax.random.split(beta_key, 3)

    z_i = jax.random.bernoulli(bk1, 0.8, shape=n)
    pi_i = tau1 * jax.random.exponential(bk2, shape=n)

    delta_i = tau0 * jax.random.normal(bk3, shape=n)
    true_beta = z_i * pi_i + delta_i

    # generate the design matrix based off the coefficients
    k1, k2, k3, k4, k5 = jax.random.split(data_key, 5)

    if rho == 0.0:
        Z = jax.random.normal(k1, shape=(N, n))
    else:
        g = jax.random.normal(k1, shape=(N, 1))  # common factor
        eps = jax.random.normal(k2, shape=(N, n))  # idiosyncratic
        Z = np.sqrt(rho) * g + np.sqrt(1 - rho) * eps

    c_j = jax.random.uniform(k3, minval=-0.25, maxval=0.25, shape=n)

    eps_k = sigma2 * jax.random.normal(k4, shape=N)
    X = Z > c_j

    # add intercept that centers the logits
    alpha = -jnp.mean(X @ true_beta + eps_k)

    logits = alpha + X @ true_beta + eps_k
    p_i = sigmoid(logits)
    y = jax.random.binomial(k5, 1, p_i)

    return X, y, true_beta


# Following code generated by ChatGPT to implement methods from a
# previous project in JAX format.
def __get_mle_vhat(model):
    """
    Extract beta_hat, V_hat from a model-like object.
    Accepts the mle_results namedtuple, a tuple/list (beta, V) or
    any object exposing .beta_hat and .V_hat attributes.
    """
    try:
        bh = getattr(model, "beta_hat")
        Vh = getattr(model, "V_hat")
        return jnp.asarray(bh), jnp.asarray(Vh)
    except Exception:
        try:
            bh, Vh = model[0], model[1]
            return jnp.asarray(bh), jnp.asarray(Vh)
        except Exception:
            raise RuntimeError("Cannot extract beta_hat and V_hat from model")


@method(output=["beta_hat", "V_hat"], label="mle_logistic")
def fit_mle(X, y, max_iter=100, ridge=1e-6):
    """
    JAX IRLS for logistic MLE using a JAX loop (no Python branching).
    Runs a fixed number of iterations; ridge regularization stabilizes solves.
    """
    X = jnp.column_stack([jnp.ones((X.shape[0],)), X])
    p_dim = X.shape[1]

    def irls_step(i, beta):
        eta = X @ beta
        p = jax.nn.sigmoid(eta)
        W = p * (1.0 - p)
        XtWX = X.T @ (W[:, None] * X)
        grad = X.T @ (y - p)
        # damped SPD system
        A = XtWX + ridge * jnp.eye(p_dim)
        # solve A * delta = grad
        # prefer cholesky when possible; fall back to solve if needed
        L = jnp.linalg.cholesky(A)
        delta = cho_solve((L, True), grad)
        return beta + delta

    beta0 = jnp.zeros((p_dim,))
    beta = lax.fori_loop(0, max_iter, irls_step, beta0)

    # covariance at final p
    eta = X @ beta
    p = jax.nn.sigmoid(eta)
    W = p * (1.0 - p)
    XtWX = X.T @ (W[:, None] * X)
    A = XtWX + ridge * jnp.eye(p_dim)
    L = jnp.linalg.cholesky(A)
    V = cho_solve((L, True), jnp.eye(p_dim))

    beta_hat = beta[1:]
    V_hat = V[1:, 1:]

    # guard without Python branching; compute a mask and use it to raise via assert-like check outside JAX
    # bad = jnp.logical_or(
    #     jnp.any(jnp.diag(V_hat) >= se_threshold**2),
    #     jnp.logical_not(jnp.isfinite(V_hat).all()),
    # )
    # In pure JAX contexts, avoid raising; here we just return. If you need a hard check, do it outside vmap.
    return mle_results(beta_hat, V_hat)


# ...existing code...)


@method(output=["beta_hat", "C_star", "tau_tilde2"], label="parametricEB")
def fit_parametricEB(beta_hat, V_hat, max_iter=250, tol=1e-6):
    """
    JAX version of Parametric Empirical Bayes for first-stage MLEs.
    Inputs/outputs are jax arrays.
    """
    beta_hat = jnp.asarray(beta_hat, dtype=jnp.float64)
    V_hat = jnp.asarray(V_hat, dtype=jnp.float64)

    n = beta_hat.shape[0]
    Z = jnp.ones((n,))

    # Initialize
    tau_tilde2 = 1e-3

    lam, Q = jnp.linalg.eigh(V_hat)

    u1 = Q.T @ Z
    u_beta = Q.T @ beta_hat

    converged = False
    for it in range(max_iter):
        inv_eigh_tau = 1.0 / (lam + tau_tilde2)
        ZtWZ = jnp.sum(u1 * u1 * inv_eigh_tau)
        ZtWbeta = jnp.sum(u1 * u_beta * inv_eigh_tau)
        pi_star = ZtWbeta / ZtWZ

        e = u_beta - pi_star * u1

        trW = jnp.sum(inv_eigh_tau)
        eWe = jnp.sum(e * e * inv_eigh_tau)
        R = eWe / trW

        trWV = jnp.sum(lam * inv_eigh_tau)
        V_bar_star = trWV / trW
        tau_new = jnp.maximum(n * R / (n - 1) - V_bar_star, 1e-8)

        logger.debug(f"Iter {it}: tau^2 = {tau_new}")
        if jnp.abs(tau_new - tau_tilde2) < tol:
            tau_tilde2 = tau_new
            logger.debug(f"Converged after {it} iterations.")
            converged = True
            break
        tau_tilde2 = tau_new

    if not converged:
        raise RuntimeError("Parametric EB fitting did not converge.")

    # post-convergence
    tau = tau_tilde2
    d = lam + tau
    inv_eigh_tau = 1.0 / d
    W_star = (Q * inv_eigh_tau) @ Q.T

    c = (n - 1 - 2) / (n - 1)
    B_diag = c * (lam / (lam + tau))
    B_star = (Q * B_diag) @ Q.T

    ZtWZ = jnp.sum(u1 * u1 * inv_eigh_tau)
    ZtWbeta = jnp.sum(u1 * u_beta * inv_eigh_tau)

    pi = ZtWbeta / ZtWZ
    mu_star = pi * Z

    beta_star = B_star @ mu_star + (jnp.eye(n) - B_star) @ beta_hat

    e = beta_hat - mu_star
    be = B_star @ e
    A = 2.0 * jnp.outer(be, be) / (n - 1)

    C_star = V_hat @ (jnp.eye(n) - (n - 1) * B_star / n) + A

    Zcol = jnp.ones((n, 1))
    A_t = 1.0 / ZtWZ
    W1 = W_star @ Zcol
    diag_H = A_t * W1.squeeze()
    v_star = jnp.trace(W_star @ V_hat) / jnp.trace(W_star)
    VBs = V_hat @ B_star
    WA = W_star @ A

    adj_vars = (
        jnp.diag(V_hat)
        - (1.0 - diag_H) * jnp.diag(VBs)
        + (v_star + tau_tilde2) * jnp.diag(WA)
    )

    C_star = C_star.at[jnp.diag_indices(n)].set(adj_vars)

    if jnp.any(jnp.diagonal(C_star) <= 0):
        raise RuntimeError("Parametric EB covariance has non-positive variances.")
    elif jnp.any(jnp.diagonal(C_star) >= 10):
        raise RuntimeError("Ill formed Parametric EB covariance matrix")

    return parametricEBResults(beta_star, C_star, tau_tilde2)


@method(output=["beta_hat", "C_tilde"], label="semiBayes")
def fit_semiBayes(model, tau2=1.0):
    """
    JAX Semi-Bayes estimator. Accepts model returned by fit_mle (or any object
    from which beta_hat and V_hat can be extracted).
    """
    beta_hat, V_hat = __get_mle_vhat(model)
    beta_hat = jnp.asarray(beta_hat, dtype=jnp.float64)
    V_hat = jnp.asarray(V_hat, dtype=jnp.float64)

    n = beta_hat.shape[0]
    p = 1
    Z = jnp.ones((n, 1))

    W = jnp.linalg.solve(V_hat + tau2 * jnp.eye(n), jnp.eye(n))
    B = W @ V_hat

    A_t = jnp.linalg.solve(Z.T @ W @ Z, jnp.eye(p))
    pi_tilde = A_t @ (Z.T @ W @ beta_hat)
    mu_tilde = Z @ pi_tilde
    C_tilde = V_hat @ (jnp.eye(n) - (n - p) * B / n)

    beta_tilde = B @ mu_tilde + (jnp.eye(n) - B) @ beta_hat

    H_tilde = Z @ A_t @ Z.T @ W
    var_adjusted = jnp.diagonal(V_hat) - (1.0 - jnp.diagonal(H_tilde)) * jnp.diagonal(
        V_hat @ B
    )

    C_tilde = C_tilde.at[jnp.diag_indices(n)].set(var_adjusted)

    return semiBayesResults(beta_tilde, C_tilde)


# END ChatGPT generation
