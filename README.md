# Untitled Simulation Framework

Readme initially generated by Google Gemini based on synthesis of the report I wrote
in the `docs` folder.

## Introduction

A minimal, Pythonic simulation framework designed to manage and reproduce complex simulation studies, with a focus on high-performance execution via JAX's vectorization capabilities.

Main Features:
- JAX Vectorization: Automatically batches and runs Data Generating Processes (DGPs) and Methods using jax.vmap for efficient, parallel execution across CPU/GPU, eliminating manual looping over simulations. 
- Reproducibility: Manages randomization through explicit key/seed handling for guaranteed results. 
- Data Persistence: "Invisible" caching layer to prevent re-running expensive DGPs or Method fitting between sessions.
- Method Evaluation: Structured pipeline for comparing methods against ground truth using user-defined evaluators.
- Minimal API: Uses simple function decorators (@dgp, @method, etc.) to define components, mirroring the familiar structure of R simulation packages.
- Flexible Backend: Supports both JAX and NumPy user functions.

## Installation

This package was made using the `uv` package and the author highly recommends
it's use. For simply using the package, run the following command from the 
root directory.

```bash
uv sync
```

If you require testing support, install the package with the `dev` extra:

```bash
uv sync --extra dev # testing tools like pytest
# or
uv sync --extra cuda # for cuda support; may not work

# or 
uv sync --extra dev --extra cuda  # for both
```

If you prefer the more traditional route, you may run

```bash
pip install -r requirements.txt
# or
pip install -r requirements-dev.txt  # for testing tools like pytest
```

The simulation framework runs in a simple pipeline tied together by function decorators and the central run_simulations orchestrator.

## Components 

Define your simulation components using simple decorators from `src.decorators`. The decorator inerface is supported
for data generating processes, methods, and evaluators. Outputs are automatically named and mapped as inputs to the next step.

Plotters come from a factory function, `src.plotters.create_plotter_fn`. 

### Data Generating Process (DGP)

Functions define the data and ground truth parameters.

```python

from simtools.decorators import dgp
import jax.random

@dgp(outputs=["X", "y", "beta"], label="LinearRegressionDGP")
def linear_regression_dgp(key, n, p, dist="normal"):
    """Generates data from a linear regression model."""
    key, subkey = jax.random.split(key)
    # ... logic to generate X, y, beta ...
    return X, y, beta
    
```

### Methods

Functions define the estimator/method that consumes DGP outputs.

```python

from simtools.decorators import method
import jax.numpy as jnp

@method(outputs="beta_hat", label="OLS")
def ols_regression(X, y):
    # X and y are automatically mapped from the DGP output names
    beta_hat = jnp.linalg.inv(X.T @ X) @ X.T @ y
    return beta_hat
```


## Run Simulations

Invoke `run_simulations` with mappings of functions and their parameter spaces. 
The system handles parameter combinations, simulation loops, execution, and caching.


```python
import seaborn as sns
from src.plotters import create_plotter_fn
from src.runners import run_simulations
from src.evaluators import rmse, bias, mae  # the three basic ones we started with
import jax

key = jax.random.key(20250607)

data, methods, evaluations, plots = run_simulations(
    key,
    dgp_mapping=[
        (data_fn, {"n": [100, 200], "p": [5, 10, 20, 50], "dist": ["normal", "t"]}),
    ],
    method_mapping=[
        (method_fn, {"alpha": [0.01, 0.1, 0.5, 1.0, 2.0]}),
    ],
    evaluators=[rmse, bias, mae],
    plotters=[
        (
            rmse,
            create_plotter_fn(
                x="p", hue="method", col="dist", plot_class=sns.lineplot,
            ),
        ),
        # ... other plotters ...
    ],
    targets=["beta"], # Target parameter to evaluate (e.g., compare 'beta' vs 'beta_hat')
    n_sims=50,
    simulation_dir="./sim_results/example", # Enables caching/persistence
)
```

Full example usages can be found in the `tests/` and `example/` directories. The tests, while not completely comprehensive, provide a good overview of the package functionality and serve as good starting point for calling the package.

To test the full simulation pipeline, run the example script:

```bash
# run from the root directory
PYTHONPATH=. python ./example/ridge_example.py
```

This should give you an idea of what the package actually does.

## Vectorization and Performance

The core speed benefit comes from how the package executes the n_sims replications:

JAX Backend (Recommended): The system automatically wraps your DGP and Method functions using jax.vmap to process the entire batch of `Nsims` simultaneously. This is highly efficient and utilizes accelerators (GPU/TPU) when available, but suffers from compilation overhead.

### NumPy Backend

`NumPy` functions are supported but cannot benefit from the automatic jax.vmap optimization. The system resorts to a Python list comprehension/loop over the `Nsims` dimension for execution. While this works for arbitrary functions, it will be significantly slower for large `Nsims` compared to JAX.

### JAX Backend

While JAX has compilation overhead, timing results show that for simulation studies involving medium-to-large Nsims (e.g., >100-1000 replications), the speedup gained from JAX's vectorization overcomes the overhead. This depends a lot on your platform.

![JAX vs NumPy Timing Comparison](docs/simulation_timings.png)

## Plotting

The package includes a flexible plotting system that allows users to define custom plotters based on evaluation metrics, but is in early development and may not be fully featured. The plots are titled and labelled automatically based on the simulation parameters,
but this part of the package could use some work.

![RMSE Plot](./docs/rmse_beta_alpha_vs_rmse.png)

## Data Persistence and Caching

The package provides a reliable, "invisible" caching layer by saving intermediate results to disk immediately. This is done
through a subclassing of the `MutableMapping` abstract class, and the corresponding classes are `src.utils.DiskDict` and `src.utils.ImageDict`.

To use these, provide a directory upon initialization:

```python
DataDict("./simulation_results")
```
Note that this is handled automatically for you when you provide a `simulation_dir` to `run_simulations`, and the 
resulting dictionaries that are passed back from the results of `run_simulations` are `DiskDict` instances.

For serialization, the package uses the `dill` library for robust serialization 
of arbitrary Python objects, including code definitions. This ensures 
reproducibility more effectively than `pickle` on average due to the wider array of objects it can handle.

To access previously generated data, simply initialize the `DataDict` to a directory containing existing simulation outputs; the data will be loaded directly into memory without recalculation when accessed. Note that by default, `DGPs` are stored in the `/data/` subdirectory, while `Methods` are stored in the `/methods/` subdirectory within the specified `simulation_dir`. When re-using a directory, you'll need
to specify these subdirectories accordingly, as the filenames become the keys for retrieval.

# Contribution

This package is currently in its Minimum Viable Product (MVP) stage as part of a class project. Contributions and feedback are welcome, but users should be aware that the API and features may change as development continues, and be cautious when utilizing it for your own
work.
